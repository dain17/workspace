{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = '/Users/iijima.s.ad/git/JASen/word2vec_100.txt'\n",
    "comment_folder = '/Users/iijima.s.ad/git/article-extractor/articles/fox/drones/*'\n",
    "comment_files = '/Users/iijima.s.ad/git/article-extractor/articles/fox/drones/*/comments.json'\n",
    "output_folder = '/Users/iijima.s.ad/git/workspace/articles/fox/drones/'\n",
    "\n",
    "origin_articles = '/Users/iijima.s.ad/git/workspace/text/data/*'\n",
    "ner_article = '/Users/iijima.s.ad/git/workspace/text/NER_article2.txt'\n",
    "ner_insert_article = '/Users/iijima.s.ad/git/workspace/text/NER_insert_article.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iijima.s.ad/.pyenv/versions/3.9.4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['tok2vec', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_replies = False\n",
    "accept_labels = set(nlp.get_pipe('ner').labels)\n",
    "# accept_labels = {'PERSON'}\n",
    "# accept_labels = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "p = re.compile('\\s+')\n",
    "def trim(sentence: str):\n",
    "    return p.sub(' ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "test_paths = set()\n",
    "\n",
    "for path in random.sample(glob.glob(comment_folder), k=50):\n",
    "    with open(os.path.join(path, 'info.json'), 'r') as f:\n",
    "        comment_count = float(json.load(f)['comment_count'])\n",
    "        if comment_count >= 10 and len(test_paths) < 10:\n",
    "            test_paths.add(path + 'comment.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "for folder_path in glob.glob(comment_folder):\n",
    "    sentence_list = []\n",
    "\n",
    "    if not os.path.exists(os.path.join(folder_path, 'comments.json')):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(folder_path, 'comments.json'), 'r') as comment_json:\n",
    "        comment_stack: list = json.load(comment_json)\n",
    "    \n",
    "    while len(comment_stack) > 0:\n",
    "        comment = comment_stack.pop()\n",
    "\n",
    "        if 'context' not in comment:\n",
    "            continue\n",
    "\n",
    "        if 'replies' in comment and use_replies:\n",
    "            comment_stack += comment['replies']\n",
    "\n",
    "        doc = nlp(trim(comment['context']))\n",
    "        tokens = list(doc)\n",
    "        ents_dict = {ent.start: ent for ent in doc.ents}\n",
    "        words = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i in ents_dict and ents_dict[i].label_ in accept_labels:\n",
    "                ent = ents_dict[i]\n",
    "                words.append(f'[[{ent.label_}]]')\n",
    "                i = ent.end\n",
    "            else:\n",
    "                words.append(tokens[i].text.lower())\n",
    "                i += 1\n",
    "        \n",
    "        sentence_list.append(' '.join(words))\n",
    "\n",
    "    file_base = 'NER_comments_with_replies.txt' if use_replies else 'NER_only_root.txt'\n",
    "    with open(os.path.join(folder_path, file_base), 'w') as f:\n",
    "        f.writelines([sentence + '\\n' for sentence in sentence_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 100000, time: 2022/10/14 02:16:02\n",
      "done 200000, time: 2022/10/14 02:24:30\n",
      "done 300000, time: 2022/10/14 02:32:38\n",
      "done 400000, time: 2022/10/14 02:41:18\n",
      "done 500000, time: 2022/10/14 02:49:27\n",
      "done 600000, time: 2022/10/14 02:58:12\n",
      "done 700000, time: 2022/10/14 03:06:39\n",
      "done 800000, time: 2022/10/14 03:15:04\n",
      "done 900000, time: 2022/10/14 03:23:42\n",
      "done 1000000, time: 2022/10/14 03:31:54\n",
      "done 1100000, time: 2022/10/14 03:39:48\n",
      "done 1200000, time: 2022/10/14 03:47:58\n",
      "done 1300000, time: 2022/10/14 03:56:29\n",
      "done 1400000, time: 2022/10/14 04:05:03\n",
      "done 1500000, time: 2022/10/14 04:13:35\n",
      "done 1600000, time: 2022/10/14 04:22:01\n",
      "done 1700000, time: 2022/10/14 04:30:21\n",
      "done 1800000, time: 2022/10/14 04:38:45\n",
      "done 1900000, time: 2022/10/14 04:47:08\n",
      "done 2000000, time: 2022/10/14 04:55:55\n",
      "done 2100000, time: 2022/10/14 05:04:07\n",
      "done 2200000, time: 2022/10/14 05:12:42\n",
      "done 2300000, time: 2022/10/14 05:20:40\n",
      "done 2400000, time: 2022/10/14 05:29:28\n",
      "done 2500000, time: 2022/10/14 05:37:56\n",
      "done 2600000, time: 2022/10/14 05:46:07\n",
      "done 2700000, time: 2022/10/14 05:53:58\n",
      "done 2800000, time: 2022/10/14 06:02:16\n",
      "done 2900000, time: 2022/10/14 06:10:25\n",
      "done 3000000, time: 2022/10/14 06:19:03\n",
      "done 3100000, time: 2022/10/14 06:26:51\n",
      "done 3200000, time: 2022/10/14 06:35:32\n",
      "done 3300000, time: 2022/10/14 06:43:52\n",
      "done 3400000, time: 2022/10/14 06:52:25\n",
      "done 3500000, time: 2022/10/14 07:00:54\n",
      "done 3600000, time: 2022/10/14 07:09:08\n",
      "done 3700000, time: 2022/10/14 07:17:12\n",
      "done 3800000, time: 2022/10/14 07:25:50\n",
      "done 3900000, time: 2022/10/14 07:34:37\n",
      "done 4000000, time: 2022/10/14 07:43:01\n",
      "done 4100000, time: 2022/10/14 07:50:43\n",
      "done 4200000, time: 2022/10/14 07:59:34\n",
      "done 4300000, time: 2022/10/14 08:08:17\n",
      "done 4400000, time: 2022/10/14 08:16:38\n",
      "done 4500000, time: 2022/10/14 08:25:21\n",
      "done 4600000, time: 2022/10/14 08:34:17\n",
      "done 4700000, time: 2022/10/14 08:42:26\n",
      "done 4800000, time: 2022/10/14 08:51:39\n",
      "done 4900000, time: 2022/10/14 09:00:23\n",
      "done 5000000, time: 2022/10/14 09:08:31\n",
      "done 5100000, time: 2022/10/14 09:17:23\n",
      "done 5200000, time: 2022/10/14 09:26:09\n",
      "done 5300000, time: 2022/10/14 09:34:25\n",
      "done 5400000, time: 2022/10/14 09:42:47\n",
      "done 5500000, time: 2022/10/14 09:51:11\n",
      "done 5600000, time: 2022/10/14 09:59:30\n",
      "done 5700000, time: 2022/10/14 10:07:55\n",
      "done 5800000, time: 2022/10/14 10:16:19\n",
      "done 5900000, time: 2022/10/14 10:24:04\n",
      "done 6000000, time: 2022/10/14 10:33:16\n",
      "done 6100000, time: 2022/10/14 10:41:44\n",
      "done 6200000, time: 2022/10/14 10:49:23\n",
      "done 6300000, time: 2022/10/14 10:57:52\n",
      "done 6400000, time: 2022/10/14 11:06:54\n",
      "done 6500000, time: 2022/10/14 11:15:16\n",
      "done 6600000, time: 2022/10/14 11:23:55\n",
      "done 6700000, time: 2022/10/14 11:32:07\n",
      "done 6800000, time: 2022/10/14 11:39:09\n",
      "done 6900000, time: 2022/10/14 11:47:29\n",
      "done 7000000, time: 2022/10/14 11:56:21\n",
      "done 7100000, time: 2022/10/14 12:04:48\n",
      "done 7200000, time: 2022/10/14 12:13:02\n",
      "done 7300000, time: 2022/10/14 12:20:26\n",
      "done 7400000, time: 2022/10/14 12:29:31\n",
      "done 7500000, time: 2022/10/14 12:37:55\n",
      "done 7600000, time: 2022/10/14 12:46:07\n",
      "done 7700000, time: 2022/10/14 12:54:36\n",
      "done 7800000, time: 2022/10/14 13:02:09\n",
      "done 7900000, time: 2022/10/14 13:09:44\n",
      "done 8000000, time: 2022/10/14 13:17:39\n",
      "done 8100000, time: 2022/10/14 13:26:06\n",
      "done 8200000, time: 2022/10/14 13:33:31\n",
      "done 8300000, time: 2022/10/14 13:41:11\n",
      "done 8400000, time: 2022/10/14 13:48:43\n",
      "done 8500000, time: 2022/10/14 13:56:11\n",
      "done 8600000, time: 2022/10/14 14:03:35\n",
      "done 8700000, time: 2022/10/14 14:10:46\n",
      "done 8800000, time: 2022/10/14 14:18:35\n",
      "done 8900000, time: 2022/10/14 14:26:12\n",
      "done 9000000, time: 2022/10/14 14:33:28\n",
      "done 9100000, time: 2022/10/14 14:40:57\n",
      "done 9200000, time: 2022/10/14 14:48:38\n",
      "done 9300000, time: 2022/10/14 14:57:01\n",
      "done 9400000, time: 2022/10/14 15:05:33\n",
      "done 9500000, time: 2022/10/14 15:14:10\n",
      "done 9600000, time: 2022/10/14 15:22:07\n",
      "done 9700000, time: 2022/10/14 15:30:01\n",
      "done 9800000, time: 2022/10/14 15:38:32\n"
     ]
    }
   ],
   "source": [
    "# News Article Corpus をNERにかける\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(ner_article, 'w') as output_file, open(ner_insert_article, 'w') as output_file2:\n",
    "\n",
    "    article_paths_list = [[]]\n",
    "    for article_path in glob.glob(origin_articles):\n",
    "        article_paths_list[-1].append(article_path)\n",
    "        if len(article_paths_list[-1]) % 1000 == 0:\n",
    "            article_paths_list.append([])\n",
    "\n",
    "\n",
    "    for article_paths in article_paths_list:\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for article_path in article_paths:\n",
    "            with open(article_path, 'r') as f:\n",
    "                sentences += [line.strip() for line in f.readlines() if line != '\\n']\n",
    "\n",
    "        for doc in nlp.pipe(sentences, batch_size=2000):\n",
    "            tokens = list(doc)\n",
    "            ents_dict = {ent.start: ent for ent in doc.ents}\n",
    "            words, words2 = [], []\n",
    "\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i in ents_dict and ents_dict[i].label_ in accept_labels:\n",
    "                    ent = ents_dict[i]\n",
    "                    words.append(f'[[{ent.label_}]]')\n",
    "                    words2.append(f'[[{ent.label_}]]')\n",
    "                    words2.append(ent.text.lower())\n",
    "                    i = ent.end\n",
    "                else:\n",
    "                    words.append(tokens[i].lemma_.lower())\n",
    "                    words2.append(tokens[i].lemma_.lower())\n",
    "                    i += 1\n",
    "            \n",
    "            if len(words) > 0:\n",
    "                output_file.write(' '.join(words) + '\\n')\n",
    "                output_file2.write(' '.join(words2) + '\\n')\n",
    "            \n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                print(f'done {count}, time: {datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355973\n"
     ]
    }
   ],
   "source": [
    "# 記事のコメント数を取得する\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "url_set = set()\n",
    "count, article_count = 0, 0\n",
    "\n",
    "for dir_path in glob.glob('/Users/iijima.s.ad/git/article-extractor/articles/fox/*/*/'):\n",
    "    if not os.path.exists(os.path.join(dir_path, 'info.json')):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(dir_path, 'info.json'), 'r') as f:\n",
    "        info = json.load(f)\n",
    "        url = info['URL']\n",
    "        \n",
    "    if url in url_set:\n",
    "        continue\n",
    "    url_set.add(url)\n",
    "\n",
    "    with open(os.path.join(dir_path, 'comments.json'), 'r') as f:\n",
    "        stack = [a for a in json.load(f)]\n",
    "\n",
    "    comment_count = 0\n",
    "    while len(stack) > 0:\n",
    "        comment_count += 1\n",
    "        comment = stack.pop()\n",
    "        if 'replies' in comment:\n",
    "            stack += comment['replies']\n",
    "\n",
    "    # if comment_count > 10:\n",
    "    article_count += 1\n",
    "    count += comment_count\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article.txtをNERにかける\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "for path in glob.glob('/Users/iijima.s.ad/git/article-extractor/articles/fox/*/*/article.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        sentences = [line.strip() for line in f.readlines() if line.strip() != '']\n",
    "\n",
    "    article_ner_file = path.replace('article.txt', 'article_ner.txt')\n",
    "    article_joint_file = path.replace('article.txt', 'article_joint.txt')\n",
    "    with open(article_ner_file, 'w') as output_file, open(article_joint_file, 'w') as output_file2:\n",
    "        for doc in nlp.pipe(sentences, batch_size=2000):\n",
    "            tokens = list(doc)\n",
    "            ents_dict = {ent.start: ent for ent in doc.ents}\n",
    "            words, words2 = [], []\n",
    "\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i in ents_dict and ents_dict[i].label_ in accept_labels:\n",
    "                    ent = ents_dict[i]\n",
    "                    words.append(f'[[{ent.label_}]]')\n",
    "                    words2.append(f'[[{ent.label_}:::{ent.text.lower()}]]')\n",
    "                    i = ent.end\n",
    "                else:\n",
    "                    words.append(tokens[i].lemma_.lower())\n",
    "                    words2.append(tokens[i].lemma_.lower())\n",
    "                    i += 1\n",
    "            \n",
    "            if len(words) > 0:\n",
    "                output_file.write(' '.join(words) + '\\n')\n",
    "                output_file2.write(' '.join(words2) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('3.9.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddf81bc3a80e275d0a0be66a828722ecd8b6d7da6398175ac1c4336a544fb459"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
