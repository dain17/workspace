{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = '/Users/iijima.s.ad/git/JASen/word2vec_100.txt'\n",
    "comment_folder = '/Users/iijima.s.ad/git/article-extractor/articles/fox/drones/*'\n",
    "comment_files = '/Users/iijima.s.ad/git/article-extractor/articles/fox/drones/*/comments.json'\n",
    "output_folder = '/Users/iijima.s.ad/git/workspace/articles/fox/drones/'\n",
    "\n",
    "train_files = '/Users/iijima.s.ad/git/article-extractor/articles/fox/*/*/NER_comments.txt'\n",
    "source_file = '/Users/iijima.s.ad/git/workspace/text/NER_article2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from gensim.models import word2vec\n",
    "\n",
    "word_list = []\n",
    "\n",
    "for train_file in glob.glob(train_files):\n",
    "    with open(train_file, 'r') as f:\n",
    "        for sentence in f.readlines():\n",
    "            word_list.append(sentence.strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles/fox/drones/ner_train.txt', 'w') as f:\n",
    "    f.writelines([' '.join(words) + '\\n' for words in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER処理済みの記事をWord2vecで学習させる\n",
    "from gensim.models import word2vec\n",
    "\n",
    "word_list = []\n",
    "\n",
    "with open(source_file, 'r') as f:\n",
    "    for sentence in f.readlines():\n",
    "        word_list.append([word for word in sentence.strip().split(' ') if word != '' and len(word) < 50])\n",
    "\n",
    "model = word2vec.Word2Vec(word_list, workers=20, vector_size=100, min_count=5, window=5, epochs=5)\n",
    "model.save(\"models/ner_article5.model\")\n",
    "model.wv.save_word2vec_format(\"models/wordvec/ner_article5.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER処理済みのコメントを使ってWord2vecを追加学習させる\n",
    "import gensim\n",
    "\n",
    "word_add_list = []\n",
    "with open('/Users/iijima.s.ad/git/workspace/articles/fox/ner_train2.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        word_add_list.append([word for word in line.strip().split(' ') if word != '' and len(word) < 50])\n",
    "\n",
    "model.train(word_add_list, total_examples=sum(len(s) for s in word_add_list), epochs=5)\n",
    "model.save('models/ner_tuned.model')\n",
    "model.wv.save_word2vec_format('models/wordvec/ner_tuned2.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "base_wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/iijima.s.ad/git/JASen/word2vec_100.txt', binary=False)\n",
    "word_trimed_list = []\n",
    "for w in model.wv.index_to_key:\n",
    "    if w in base_wv or re.match(r'\\[\\[\\w+\\]\\]', w) is not None:\n",
    "        word_trimed_list.append(f'{w} {\" \".join([str(v) for v in model.wv[w]])}\\n')\n",
    "\n",
    "with open('ner_article_wv3.txt', 'w') as f:\n",
    "    f.write(f'{len(word_trimed_list)} 100\\n')\n",
    "    f.writelines(word_trimed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/iijima.s.ad/git/workspace/models/wordvec/ner_article_wv5.txt', 'r') as origin_wv:\n",
    "    tmp_data = [line.strip().split(' ') for line in origin_wv.readlines()[1:]]\n",
    "\n",
    "with open('/Users/iijima.s.ad/git/workspace/models/wordvec/ner_article_wv3.txt', 'r') as f:\n",
    "    tmp_data2 = {line.strip().split(' ')[0] for line in f.readlines()[1:]}\n",
    "\n",
    "diffrence_data = [l for l in tmp_data if l[0] not in tmp_data2]\n",
    "with open('/Users/iijima.s.ad/git/workspace/models/sub/removed.txt', 'w') as f:\n",
    "    f.write('\\n'.join([' '.join(l) for l in diffrence_data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/iijima.s.ad/git/workspace/models/wordvec/ner_article_wv5.txt', 'r') as f:\n",
    "    wv_data5 = f.readlines()[1:]\n",
    "\n",
    "i, name = 0, 1\n",
    "while i < len(diffrence_data):\n",
    "    with open(f'/Users/iijima.s.ad/git/workspace/models/sub/ner_tmp{name}', 'w') as f:\n",
    "        tmp_data4 = wv_data5 + [' '.join(l) + '\\n' for l in diffrence_data[i:i+1000]]\n",
    "        f.write(f'{len(tmp_data4)} 100\\n')\n",
    "        f.writelines(tmp_data4)\n",
    "    i += 1000\n",
    "    name += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = 10\n",
    "vectors = diffrence_data[0:1000]\n",
    "i, name = 0, 100\n",
    "length = math.ceil(len(vectors) / N)\n",
    "\n",
    "for j in range(N):\n",
    "    with open(f'/Users/iijima.s.ad/git/workspace/models/sub/ner_tmp{name}', 'w') as f:\n",
    "        tmp_data4 = wv_data5 + [' '.join(l) + '\\n' for l in vectors[i:i+length]]\n",
    "        f.write(f'{len(tmp_data4)} 100\\n')\n",
    "        f.writelines(tmp_data4)\n",
    "        i += length\n",
    "        name += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/iijima.s.ad/git/workspace/models/wordvec/ner_article_wv5.txt', 'r') as f:\n",
    "    wv_data5 = f.readlines()[1:]\n",
    "\n",
    "with open(f'/Users/iijima.s.ad/git/workspace/models/sub/ner_tmp0', 'w') as f:\n",
    "    wv_data0 = [l for l in wv_data5 if len(l.strip().split(' ')) == 101]\n",
    "    f.write(f'{len(wv_data0)} 100\\n')\n",
    "    f.writelines(wv_data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load('models/ner_article4.model')\n",
    "model.wv.save_word2vec_format('models/wordvec/ner_article4.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ugly [[WORK_OF_ART]] 0.0732, [[LOC]] 0.0466, [[PERSON]] 0.0353, \n",
      "miserable [[GPE]] 0.0505, [[EVENT]] 0.0347, [[NORP]] -0.0285, \n",
      "negative [[PRODUCT]] -0.0041, [[LOC]] -0.0104, [[GPE]] -0.0250, \n",
      "fearful [[NORP]] 0.0436, [[GPE]] 0.0141, [[LOC]] -0.0117, \n",
      "disagreeable [[PERSON]] 0.0244, [[LOC]] -0.0066, [[NORP]] -0.0354, \n",
      "harsh [[NORP]] 0.0467, [[LOC]] 0.0290, [[GPE]] 0.0290, \n",
      "angry [[NORP]] 0.1411, [[PERSON]] 0.0695, [[ORG]] 0.0462, \n",
      "resentful [[NORP]] 0.0689, [[LOC]] 0.0276, [[PERSON]] -0.0020, \n",
      "improper [[ORG]] -0.0121, [[PRODUCT]] -0.0140, [[NORP]] -0.0384, \n",
      "ill [[NORP]] 0.0357, [[EVENT]] -0.0085, [[PERSON]] -0.0271, \n",
      "upset [[PERSON]] 0.0926, [[NORP]] 0.0818, [[ORG]] 0.0670, \n",
      "humble [[WORK_OF_ART]] 0.1248, [[PERSON]] 0.1199, [[LOC]] 0.0655, \n",
      "deny [[ORG]] 0.0281, [[NORP]] 0.0189, [[PERSON]] 0.0047, \n",
      "fear [[LOC]] 0.0792, [[GPE]] 0.0569, [[NORP]] 0.0509, \n"
     ]
    }
   ],
   "source": [
    "tags = ['[[PERSON]]', '[[ORG]]', '[[NORP]]', '[[FAC]]', '[[LOC]]', '[[GPE]]', '[[PRODUCT]]', '[[WORK_OF_ART]]', '[[EVENT]]']\n",
    "\n",
    "for word in ['ugly', 'miserable', 'negative', 'fearful', 'disagreeable', 'harsh', 'angry', 'resentful', 'improper', 'ill', 'upset', 'humble', 'deny', 'fear']:\n",
    "    sorted_similarities = sorted([(model.wv.similarity(word, t), t) for t in tags], key=lambda t: t[0], reverse=True)\n",
    "    print(word + ' ', end='')\n",
    "    for s in sorted_similarities[:3]:\n",
    "        print(f'{s[1]} {s[0]:.4f}, ', end='')\n",
    "    print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mywork')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5ee4ea3619771fd4165c26fd5035315b9de9bea138d8ebbe38c5f5aaa6c4622"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
