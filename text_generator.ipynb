{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_comment_list(comments):\n",
    "    replies, stack = [], [root for root in comments if 'error' not in root]\n",
    "    roots = [root for root in comments if 'error' not in root]\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        comment = stack.pop()\n",
    "        if 'error' not in comment:\n",
    "            replies += comment['replies']\n",
    "            stack += comment['replies']\n",
    "\n",
    "    return roots, [reply for reply in replies if 'error' not in reply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['tok2vec', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER処理の関数\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "accept_labels = set(nlp.get_pipe('ner').labels)\n",
    "# accept_labels = {'PERSON'}\n",
    "# accept_labels = set()\n",
    "\n",
    "p = re.compile('\\s+')\n",
    "def trim(sentence: str):\n",
    "    return p.sub(' ', sentence)\n",
    "\n",
    "def ner(sentences):\n",
    "    sentence_list, joint_sentence_list = [], []\n",
    "\n",
    "    for doc in nlp.pipe([trim(sentence) for sentence in sentences], batch_size=1000):\n",
    "        tokens = list(doc)\n",
    "        ents_dict = {ent.start: ent for ent in doc.ents}\n",
    "        words, joint_words = [], []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i in ents_dict and ents_dict[i].label_ in accept_labels:\n",
    "                ent = ents_dict[i]\n",
    "                words.append(f'[[{ent.label_}]]')\n",
    "                joint_words.append(f'[[{ent.label_}:::{ent.text.lower()}]]')\n",
    "                i = ent.end\n",
    "            else:\n",
    "                words.append(tokens[i].lemma_.lower())\n",
    "                joint_words.append(tokens[i].lemma_.lower())\n",
    "                i += 1\n",
    "\n",
    "        sentence_list.append(' '.join(words))\n",
    "        joint_sentence_list.append(' '.join(joint_words))\n",
    "\n",
    "    return sentence_list, joint_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments.jsonのコメント文にNERを処理する\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "url_set = set()\n",
    "\n",
    "for dir_path in glob.glob('/Users/iijima.s.ad/git/article-extractor/articles/fox/military/*/'):\n",
    "    if not os.path.exists(os.path.join(dir_path, 'info.json')) or os.path.exists(os.path.join(dir_path, 'NER_roots.json')):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(dir_path, 'info.json'), 'r') as f:\n",
    "        url = json.load(f)['URL']\n",
    "        \n",
    "    if url in url_set:\n",
    "        continue\n",
    "    url_set.add(url)\n",
    "\n",
    "    with open(os.path.join(dir_path, 'comments.json'), 'r') as f:\n",
    "        roots, replies = tree_to_comment_list(json.load(f))\n",
    "\n",
    "    ner_roots, joint_roots = ner([root['context'] for root in roots])\n",
    "    ner_replies, joint_replies = ner([reply['context'] for reply in replies])\n",
    "\n",
    "    file_pairs = [('NER_roots.txt', ner_roots), ('joint_roots.txt', joint_roots), ('NER_comments.txt', ner_roots + ner_replies), ('joint_comments.txt', joint_roots + joint_replies)]\n",
    "    for base_name, sentences in file_pairs:\n",
    "        with open(os.path.join(dir_path, base_name), 'w') as f:\n",
    "            f.write('\\n'.join(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /Users/iijima.s.ad/git/article-extractor/articles/fox/ | grep 'NER_roots.txt' | xargs cat > /Users/iijima.s.ad/git/workspace/articles/fox/ner_train3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nkf -w --overwrite /Users/iijima.s.ad/git/workspace/articles/fox/insert_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  632240 15709992 102484322\n",
      "    2748    2748  399642\n"
     ]
    }
   ],
   "source": [
    "!find /Users/iijima.s.ad/git/article-extractor/articles/fox | grep 'comments.json$' | xargs cat | grep 'context' | wc\n",
    "!find /Users/iijima.s.ad/git/article-extractor/articles/fox | grep 'comments.json$' | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for path in `find /Users/iijima.s.ad/git/article-extractor/articles/fox | grep 'joint_roots.txt'`\n",
    "do\n",
    "    sed -E 's@:::([^]]+)\\]\\]@\\]\\] \\1@g' ${path} > `echo ${path} | sed -e 's/joint_roots/insert_roots/g'`\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/wordvec/insert_article.vec', 'r') as input_file, open('./models/wordvec/insert_article2.vec', 'w') as output_file:\n",
    "    tmp_list = [l for l in input_file.readlines()[1:] if len(l.strip().split(' ')) == 101]\n",
    "    output_file.write(f'{len(tmp_list)} 100\\n')\n",
    "    output_file.writelines(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('./models/wordvec/insert_article2.vec', 'r') as input_file:\n",
    "    print(len([l for l in input_file.readlines()[1:] if len(l.strip().split(' ')) != 101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.txtをNERのタグごとに分割する\n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "with open('/Users/iijima.s.ad/git/JASen/datasets/fox/test_insert.txt', 'r') as input_file, open('/Users/iijima.s.ad/git/article-extractor/articles/fox/drones/biden-admin-plan-counter-threats-drones/joint_roots.txt', 'r') as sentence_file:\n",
    "    sentence_id = 0\n",
    "\n",
    "    for line, sentence in zip(input_file.readlines(), sentence_file.readlines()[:30]):\n",
    "        _, aspect, senti, __ = line.strip().split('\\t')\n",
    "        for m in re.finditer(r'\\[\\[PERSON:::.*?\\]\\]', sentence.strip()):\n",
    "            start, end = m.span()\n",
    "            tmp_sentence = re.sub(r'\\[\\[[^:]+:::(.*?)\\]\\]', r'\\1', sentence[:start]) + '[[PERSON]]' + re.sub(r'\\[\\[[^:]+:::(.*?)\\]\\]', r'\\1', sentence[end:])\n",
    "            sentences.append(f'{sentence_id}\\t{aspect}\\t{senti}\\t{tmp_sentence.strip()}\\n')\n",
    "            sentence_id += 1\n",
    "        if '[[PERSON:::' not in sentence:\n",
    "            tmp_sentence = re.sub(r'\\[\\[[^:]+:::(.*?)\\]\\]', r'\\1', sentence)\n",
    "            sentences.append(f\"{sentence_id}\\t{aspect}\\t{senti}\\t{tmp_sentence}\")\n",
    "            sentence_id += 1\n",
    "\n",
    "with open('/Users/iijima.s.ad/git/JASen/datasets/fox/test.txt', 'w') as f:\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint.txtから行が1つのタグの文章に置き換える関数\n",
    "def replace_to_split_sentences(lines):\n",
    "    sentences = []\n",
    "    for sentence in lines:\n",
    "        for m in re.finditer(r'\\[\\[([^:]+):::.*?\\]\\]', sentence):\n",
    "            start, end = m.span()\n",
    "            tmp_sentence = re.sub(r'\\[\\[[^:]+:::(.*?)\\]\\]', r'\\1', sentence[:start]) + '[[' + m.group(1) + ']]' + re.sub(r'\\[\\[[^:]+:::(.*?)\\]\\]', r'\\1', sentence[end:])\n",
    "            sentences.append(tmp_sentence)\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 's を除く関数\n",
    "def trim_s(word):\n",
    "    return word[:-2] if word[-2:] == \"'s\" else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointからタグtxtに書き換える\n",
    "\n",
    "import glob\n",
    "\n",
    "for path in glob.glob('/Users/iijima.s.ad/git/article-extractor/articles/fox/*/*/article_joint.txt'):\n",
    "    tag_path = path.replace('article_joint.txt', 'article_tag.txt')\n",
    "    with open(tag_path, 'w') as f, open(path, 'r') as joint:\n",
    "        f.writelines(replace_to_split_sentences(joint.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_trainをタグtxtに変換する\n",
    "\n",
    "with open('./articles/fox/joint_train.txt', 'r') as train_file, open('./articles/fox/tag_train.txt', 'w') as f:\n",
    "    f.writelines(replace_to_split_sentences(train_file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mywork')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5ee4ea3619771fd4165c26fd5035315b9de9bea138d8ebbe38c5f5aaa6c4622"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
